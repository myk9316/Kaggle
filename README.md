# Kaggle 종료 대회 코드 필사

# Kaggle Website
- https://www.kaggle.com/

# Description
- 이미 종료된 Kaggle 대회 코드를 뜯어보기 
- 대회 선정은 [이유한님의 캐글 커널 커리큘럼](https://aifrenz.github.io/present_file/%EC%BB%A4%EB%84%90%EC%BB%A4%EB%A6%AC%ED%81%98%EB%9F%BC.pdf)을 참고 & 개인적으로 관심있는 분야 
- 똑같이 3번 따라쓰기

# Project
## 1. Titanic - Machine Learning from Disaster (Binary classification : Tabular data)
- [타이타닉 튜토리얼 1 - Exploratory data analysis, visualization, machine learning](https://kaggle-kr.tistory.com/17?category=868316)
- [EDA To Prediction(DieTanic)](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)
- [Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)
- [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)

## 2. Porto Seguro’s Safe Driver Prediction (Binary classification : Tabular data)
- [Data Preparation & Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration)
- [Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)
- [XGBoost CV (LB .284)](https://www.kaggle.com/aharless/xgboost-cv-lb-284)
- [Porto Seguro Exploratory Analysis and Prediction](https://www.kaggle.com/gpreda/porto-seguro-exploratory-analysis-and-prediction)

## 3. Bike Sharing Demand
- [EDA & Ensemble Model (Top 10 Percentile)](https://www.kaggle.com/code/viveksrinivasan/eda-ensemble-model-top-10-percentile)
- [BIKE SHARING DEMAND](https://www.kaggle.com/code/rajmehra03/bike-sharing-demand-rmsle-0-3194)
- [데이터 전처리에 집중한 자전거 수요예측하기 (for beginner)](https://www.kaggle.com/code/kwonyoung234/for-beginner)
- [Comprehensive EDA with XGBoost (Top 10 percentile)](https://www.kaggle.com/code/miteshyadav/comprehensive-eda-with-xgboost-top-10-percentile)
- [Bike Sharing(Feature Engineering)](https://www.kaggle.com/code/fatmakursun/bike-sharing-feature-engineering)
- [XG Boost, Random Forest, Ridge & Lasso Regression](https://www.kaggle.com/code/carolineecc/xg-boost-random-forest-ridge-lasso-regression)

## 4. Home Credit Default Risk (Binary classification : Tabular data)
- [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook)
- [Introduction to Manual Feature Engineering](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering/notebook)
- [Stacking Test-Sklearn, XGBoost, CatBoost, LightGBM](https://www.kaggle.com/code/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm/script)
- [LightGBM 7th place solution](https://www.kaggle.com/code/jsaguiar/lightgbm-7th-place-solution/script)

## 5. Costa Rican Household Poverty Level Prediction (Multi-class classification : Tabular data)
- [A Complete Introduction and Walkthrough](https://www.kaggle.com/code/willkoehrsen/a-complete-introduction-and-walkthrough/notebook)
- [3250feats->532 feats using shap](https://www.kaggle.com/code/youhanlee/3250feats-532-feats-using-shap-lb-0-436/notebook)
- [XGBoost](https://www.kaggle.com/code/skooch/xgboost/notebook)
